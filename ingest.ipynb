{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "penncli = PennsieveClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-29T22:46:50.374403Z'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([v[\"content\"][\"updatedAt\"] for v in penncli.get_dataset(did)[\"children\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodeId': 'N:package:cc6c1783-4eb4-4a0a-b723-247827ebeb4e',\n",
       " 'fileName': 'naveen_paper_B733_T7L_A_LEVEL_3_fascicles.csv',\n",
       " 'packageName': 'naveen_paper_B733_T7L_A_LEVEL_3_fascicles.csv',\n",
       " 'path': ['derivative', 'sam-l', 'sam-l-seg-t7', 'sam-l-seg-t7-LEVEL_3'],\n",
       " 'url': 'https://prd-sparc-storage-use1.s3.amazonaws.com/O367/D3090/be5e20cb-86e2-41ea-a72f-e29d61354c96/cc6c1783-4eb4-4a0a-b723-247827ebeb4e?response-content-disposition=attachment%3B%20filename%3D%22naveen_paper_B733_T7L_A_LEVEL_3_fascicles.csv%22&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEH8aCXVzLWVhc3QtMSJHMEUCIAPch%2BauQrPQv21R0RXeIldFPBezXJBak%2FnCmQ%2F317bTAiEA%2Fk%2B653R0GVTYYy3RlaE5nnPR3oEjUkHPZ%2FzfzNWZFCAqigQIp%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgw3NDA0NjMzMzcxNzciDN3ESJWEi1SigEZhZireA4etY0qpkpsnV%2FLZKvXvfmPzVQZ%2F96LEJUqCnAEttICbb%2BmKLcIqKm4AvjziRHltIQmsmeFHLazhcNzCZ%2Bhgz1o%2BuhDFeFWimy3UNxwKGXn6g0sc5sCMytJBXKKL8Omq5tSDpVVn7quipiIlZbM1S0nLed6LNZ4ScNHhSA%2Fues8byTAHTBZXV3OUML9AFrAcggwvoxdwCM28G4lS%2B0vY4vM0%2BT84%2BKhwiGAEjiSNuVJzgG7%2BVCywWoFxxrw9G0ckZznZb7h%2BzMyhg9Z7HmgfIAIQfLxXTD1%2FG0FfyQ0%2BpNd2CPhSbpkfIkN8LjGbjtAA1qIVcnZJpAWoFObmAnPBSPBggrb86Ugrt7QfqC9kRoxwLlUECmfMPSE9yRZrMFmm63vRMWCcJPsgTB3pQYVOWorUZTLCkeZvtI6MhwEJzqbalDE%2FRCWrQhmQhZUYTqFhaDdcGEXEUWHnkZ%2FNhez7WSGev2GjIgw1URhhAXxQfjmvUQ2gXAo24ihLhvytqug6zQwgJ5jyy9EacQCXdtHbQzrG%2FWrBk6VPYaN%2FSov4LFk6FPWgx9lO3Q249ksJmKh7JjsMSppUe9vh5Jf%2Bnnp%2BjvUrHzK5Orv00RVnl7oDmBsZBK6uSAi7bskk%2FZfRKH0w5uuLtwY6pQEAZsm%2BeoKoUxJqPoB3ZKFCiludpY4TuNPFOF8%2FJYyIjR%2FVV0VEIfDmKH5YJef8KvQI7vEVsxq6oFW2iPhLXNiHf8GAR0AUSv0bDRC1QU0S0royAOv5CyambGBJSFIVXLmKSoHIgyUrl5X7EshnOe648LLvWaSbyJHa2%2F6orbvd2dHyO9%2FjfrOJYRP%2BNrZlclRi48R3bRz9Ub93MHDNhqsefPmlndM%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240912T142530Z&X-Amz-SignedHeaders=host&X-Amz-Expires=10800&X-Amz-Credential=ASIA2YZYN33M3LYCBBHW%2F20240912%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=7512d6c9dc110c72f1ef35f9978edbb4057cdb5903729d270849db99ad895825',\n",
       " 'size': 26289}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# penncli = PennsieveClient()\n",
    "did = \"N:dataset:aa43eda8-b29a-4c25-9840-ecbd57598afc\"\n",
    "manifest = penncli.get_dataset_manifest(did)\n",
    "manifest\n",
    "\n",
    "\n",
    "def pull_files(id_or_name: str, pattern: str | None = None) -> list[str]:\n",
    "    \"\"\"Export a dataset to a directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id_or_name : str\n",
    "        Dataset ID or name\n",
    "    output_dir : Path | str\n",
    "        Output directory\n",
    "    verbose : bool, optional\n",
    "        Prints filename being downloaded, by default True\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    url = \"https://api.pennsieve.io/packages/download-manifest\"\n",
    "    # output_dir = Path(output_dir) / id_or_name\n",
    "    # Pull dataset for root children IDs\n",
    "    dataset = penncli.get_dataset(id_or_name)\n",
    "    for child in dataset[\"children\"]:\n",
    "        payload = {\"nodeIds\": [child[\"content\"][\"nodeId\"]]}\n",
    "        # Pull tree for 1 child at a time since prebuilt s3 links only last a couple hours\n",
    "        # If all children are pulled at once, the links will expire before all files are downloaded if over 200GBs\n",
    "        # resp = penncli._post(url, payload=payload)\n",
    "        # try:\n",
    "        #     manifest = resp.json()\n",
    "        # except:\n",
    "        #     print(payload)\n",
    "        #     print(resp.text)\n",
    "        #     continue\n",
    "        manifest = penncli.get_child_manifest(\n",
    "            node_id=child[\"content\"][\"nodeId\"]\n",
    "        )\n",
    "        for filemeta in manifest[\"data\"]:\n",
    "            # filemeta[\"\"] = manifest[\"datasetId\"]\n",
    "            if Path(filemeta[\"fileName\"]).suffix in [\".csv\"]:\n",
    "                files.append(filemeta)\n",
    "    return dataset, files\n",
    "\n",
    "\n",
    "dataset, files = pull_files(did)\n",
    "fascicle_files = [v for v in files if \"fascicles\" in v[\"fileName\"].lower()]\n",
    "file = fascicle_files[-1]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fascicle</th>\n",
       "      <th>area</th>\n",
       "      <th>longest_diameter</th>\n",
       "      <th>shortest_diameter</th>\n",
       "      <th>eff_diam</th>\n",
       "      <th>c_estimate_nav</th>\n",
       "      <th>c_estimate_nf</th>\n",
       "      <th>nfibers_w_c_estimate_nav</th>\n",
       "      <th>nfibers_w_c_estimate_nf</th>\n",
       "      <th>nfibers_all</th>\n",
       "      <th>...</th>\n",
       "      <th>epi_dist</th>\n",
       "      <th>epi_dist_inv</th>\n",
       "      <th>nerve_based_area</th>\n",
       "      <th>nerve_based_perimeter</th>\n",
       "      <th>nerve_based_eff_diam</th>\n",
       "      <th>perinerium_vertices</th>\n",
       "      <th>perinerium_vertices_px</th>\n",
       "      <th>nerve_based_shortest_diameter</th>\n",
       "      <th>hull_contrs</th>\n",
       "      <th>hull_contr_areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>601840.629341</td>\n",
       "      <td>907.000466</td>\n",
       "      <td>846.340808</td>\n",
       "      <td>875.378369</td>\n",
       "      <td>0</td>\n",
       "      <td>16502</td>\n",
       "      <td>4613</td>\n",
       "      <td>21115</td>\n",
       "      <td>21115</td>\n",
       "      <td>...</td>\n",
       "      <td>515.931</td>\n",
       "      <td>0.000</td>\n",
       "      <td>477860.198</td>\n",
       "      <td>4084.885</td>\n",
       "      <td>780.020</td>\n",
       "      <td>[[3127.5   113.4 ]\\n [3160.35  113.4 ]\\n [3192...</td>\n",
       "      <td>[[20850   756]\\n [21069   756]\\n [21280   826]...</td>\n",
       "      <td>630.464</td>\n",
       "      <td>[  0.   5.  10.  15.  20.  25.  30.  35.  40. ...</td>\n",
       "      <td>[3.38965189e+06 3.29847812e+06 3.20848098e+06 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>151657.783648</td>\n",
       "      <td>566.334306</td>\n",
       "      <td>329.680645</td>\n",
       "      <td>439.427682</td>\n",
       "      <td>0</td>\n",
       "      <td>1300</td>\n",
       "      <td>1548</td>\n",
       "      <td>2848</td>\n",
       "      <td>2848</td>\n",
       "      <td>...</td>\n",
       "      <td>274.845</td>\n",
       "      <td>241.087</td>\n",
       "      <td>163417.736</td>\n",
       "      <td>1544.309</td>\n",
       "      <td>456.147</td>\n",
       "      <td>[[1488.15  480.75]\\n [1504.8   485.55]\\n [1521...</td>\n",
       "      <td>[[ 9921  3205]\\n [10032  3237]\\n [10143  3262]...</td>\n",
       "      <td>350.905</td>\n",
       "      <td>[  0.   5.  10.  15.  20.  25.  30.  35.  40. ...</td>\n",
       "      <td>[1.08945157e+06 1.03852465e+06 9.88700989e+05 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42956.807122</td>\n",
       "      <td>253.176413</td>\n",
       "      <td>208.473851</td>\n",
       "      <td>233.868137</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>289</td>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>...</td>\n",
       "      <td>239.555</td>\n",
       "      <td>276.376</td>\n",
       "      <td>51265.699</td>\n",
       "      <td>894.344</td>\n",
       "      <td>255.487</td>\n",
       "      <td>[[605.55 361.5 ]\\n [614.85 361.5 ]\\n [624.3  3...</td>\n",
       "      <td>[[4037 2410]\\n [4099 2410]\\n [4162 2410]\\n [42...</td>\n",
       "      <td>215.234</td>\n",
       "      <td>[  0.   5.  10.  15.  20.  25.  30.  35.  40. ...</td>\n",
       "      <td>[3.41771325e+05 3.12657208e+05 2.84935875e+05 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>19745.681094</td>\n",
       "      <td>177.272006</td>\n",
       "      <td>131.860742</td>\n",
       "      <td>158.559080</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>120</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>394.411</td>\n",
       "      <td>121.521</td>\n",
       "      <td>24790.376</td>\n",
       "      <td>657.547</td>\n",
       "      <td>177.663</td>\n",
       "      <td>[[728.1  470.85]\\n [734.7  469.2 ]\\n [741.   4...</td>\n",
       "      <td>[[4854 3139]\\n [4898 3128]\\n [4940 3123]\\n [49...</td>\n",
       "      <td>138.983</td>\n",
       "      <td>[ 0.  5. 10. 15. 20. 25. 30. 35. 40. 45. 50. 5...</td>\n",
       "      <td>[165269.175      144192.08666671 124780.433718...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>72513.774721</td>\n",
       "      <td>370.254924</td>\n",
       "      <td>237.445074</td>\n",
       "      <td>303.854250</td>\n",
       "      <td>0</td>\n",
       "      <td>630</td>\n",
       "      <td>695</td>\n",
       "      <td>1325</td>\n",
       "      <td>1325</td>\n",
       "      <td>...</td>\n",
       "      <td>432.613</td>\n",
       "      <td>83.318</td>\n",
       "      <td>81804.589</td>\n",
       "      <td>1104.140</td>\n",
       "      <td>322.733</td>\n",
       "      <td>[[1107.    441.45]\\n [1115.55  446.4 ]\\n [1119...</td>\n",
       "      <td>[[7380 2943]\\n [7437 2976]\\n [7462 3032]\\n [75...</td>\n",
       "      <td>250.218</td>\n",
       "      <td>[  0.   5.  10.  15.  20.  25.  30.  35.  40. ...</td>\n",
       "      <td>[545363.925      509233.56034859 474385.219163...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fascicle           area  longest_diameter  shortest_diameter    eff_diam  \\\n",
       "0         1  601840.629341        907.000466         846.340808  875.378369   \n",
       "1         2  151657.783648        566.334306         329.680645  439.427682   \n",
       "2         3   42956.807122        253.176413         208.473851  233.868137   \n",
       "3         4   19745.681094        177.272006         131.860742  158.559080   \n",
       "4         5   72513.774721        370.254924         237.445074  303.854250   \n",
       "\n",
       "   c_estimate_nav  c_estimate_nf  nfibers_w_c_estimate_nav  \\\n",
       "0               0          16502                      4613   \n",
       "1               0           1300                      1548   \n",
       "2               0            239                       289   \n",
       "3               0            108                       120   \n",
       "4               0            630                       695   \n",
       "\n",
       "   nfibers_w_c_estimate_nf  nfibers_all  ...  epi_dist  epi_dist_inv  \\\n",
       "0                    21115        21115  ...   515.931         0.000   \n",
       "1                     2848         2848  ...   274.845       241.087   \n",
       "2                      528          528  ...   239.555       276.376   \n",
       "3                      228          228  ...   394.411       121.521   \n",
       "4                     1325         1325  ...   432.613        83.318   \n",
       "\n",
       "   nerve_based_area  nerve_based_perimeter  nerve_based_eff_diam  \\\n",
       "0        477860.198               4084.885               780.020   \n",
       "1        163417.736               1544.309               456.147   \n",
       "2         51265.699                894.344               255.487   \n",
       "3         24790.376                657.547               177.663   \n",
       "4         81804.589               1104.140               322.733   \n",
       "\n",
       "                                 perinerium_vertices  \\\n",
       "0  [[3127.5   113.4 ]\\n [3160.35  113.4 ]\\n [3192...   \n",
       "1  [[1488.15  480.75]\\n [1504.8   485.55]\\n [1521...   \n",
       "2  [[605.55 361.5 ]\\n [614.85 361.5 ]\\n [624.3  3...   \n",
       "3  [[728.1  470.85]\\n [734.7  469.2 ]\\n [741.   4...   \n",
       "4  [[1107.    441.45]\\n [1115.55  446.4 ]\\n [1119...   \n",
       "\n",
       "                              perinerium_vertices_px  \\\n",
       "0  [[20850   756]\\n [21069   756]\\n [21280   826]...   \n",
       "1  [[ 9921  3205]\\n [10032  3237]\\n [10143  3262]...   \n",
       "2  [[4037 2410]\\n [4099 2410]\\n [4162 2410]\\n [42...   \n",
       "3  [[4854 3139]\\n [4898 3128]\\n [4940 3123]\\n [49...   \n",
       "4  [[7380 2943]\\n [7437 2976]\\n [7462 3032]\\n [75...   \n",
       "\n",
       "   nerve_based_shortest_diameter  \\\n",
       "0                        630.464   \n",
       "1                        350.905   \n",
       "2                        215.234   \n",
       "3                        138.983   \n",
       "4                        250.218   \n",
       "\n",
       "                                         hull_contrs  \\\n",
       "0  [  0.   5.  10.  15.  20.  25.  30.  35.  40. ...   \n",
       "1  [  0.   5.  10.  15.  20.  25.  30.  35.  40. ...   \n",
       "2  [  0.   5.  10.  15.  20.  25.  30.  35.  40. ...   \n",
       "3  [ 0.  5. 10. 15. 20. 25. 30. 35. 40. 45. 50. 5...   \n",
       "4  [  0.   5.  10.  15.  20.  25.  30.  35.  40. ...   \n",
       "\n",
       "                                    hull_contr_areas  \n",
       "0  [3.38965189e+06 3.29847812e+06 3.20848098e+06 ...  \n",
       "1  [1.08945157e+06 1.03852465e+06 9.88700989e+05 ...  \n",
       "2  [3.41771325e+05 3.12657208e+05 2.84935875e+05 ...  \n",
       "3  [165269.175      144192.08666671 124780.433718...  \n",
       "4  [545363.925      509233.56034859 474385.219163...  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file[\"url\"])\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantdb.config import auth\n",
    "from sqlalchemy import create_engine\n",
    "from quantdb.utils import dbUri, isoformat, log\n",
    "from sqlalchemy.sql import text as sql_text\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "# pull in the db connection info\n",
    "dbkwargs = {\n",
    "    k: auth.get(f\"db-{k}\") for k in (\"user\", \"host\", \"port\", \"database\")\n",
    "}  # TODO integrate with cli options\n",
    "# custom user variable needed\n",
    "dbkwargs[\"dbuser\"] = dbkwargs.pop(\"user\")\n",
    "# create connection env with DB\n",
    "engine = create_engine(dbUri(**dbkwargs))\n",
    "# bool: echo me\n",
    "engine.echo = True\n",
    "# use connection env as unique session\n",
    "session = Session(engine)\n",
    "\n",
    "\n",
    "class Queries:\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "\n",
    "    def address_from_fadd_type_fadd(self, fadd_type, fadd):\n",
    "        # FIXME multi etc.\n",
    "        res = [\n",
    "            i\n",
    "            for i, in self.session.execute(\n",
    "                sql_text(\n",
    "                    \"select * from address_from_fadd_type_fadd(:fadd_type, :fadd)\"\n",
    "                ),\n",
    "                dict(fadd_type=fadd_type, fadd=fadd),\n",
    "            )\n",
    "        ]\n",
    "        if res:\n",
    "            return res[0]\n",
    "\n",
    "    def desc_inst_from_label(self, label):\n",
    "        # FIXME multi etc.\n",
    "        res = [\n",
    "            i\n",
    "            for i, in self.session.execute(\n",
    "                sql_text(\"select * from desc_inst_from_label(:label)\"),\n",
    "                dict(label=label),\n",
    "            )\n",
    "        ]\n",
    "        if res:\n",
    "            return res[0]\n",
    "\n",
    "    def desc_quant_from_label(self, label):\n",
    "        # FIXME multi etc.\n",
    "        res = [\n",
    "            i\n",
    "            for i, in self.session.execute(\n",
    "                sql_text(\"select * from desc_quant_from_label(:label)\"),\n",
    "                dict(label=label),\n",
    "            )\n",
    "        ]\n",
    "        if res:\n",
    "            return res[0]\n",
    "\n",
    "    def desc_cat_from_label_domain_label(self, label, domain_label):\n",
    "        # FIXME multi etc.\n",
    "        res = [\n",
    "            i\n",
    "            for i, in self.session.execute(\n",
    "                sql_text(\n",
    "                    \"select * from desc_cat_from_label_domain_label(:label, :domain_label)\"\n",
    "                ),\n",
    "                dict(label=label, domain_label=domain_label),\n",
    "            )\n",
    "        ]\n",
    "        if res:\n",
    "            return res[0]\n",
    "\n",
    "    def cterm_from_label(self, label):\n",
    "        # FIXME multi etc.\n",
    "        res = [\n",
    "            i\n",
    "            for i, in self.session.execute(\n",
    "                sql_text(\"select * from cterm_from_label(:label)\"),\n",
    "                dict(label=label),\n",
    "            )\n",
    "        ]\n",
    "        if res:\n",
    "            return res[0]\n",
    "\n",
    "    def insts_from_dataset_ids(self, dataset, ids):\n",
    "        return list(\n",
    "            self.session.execute(\n",
    "                sql_text(\n",
    "                    \"select * from insts_from_dataset_ids(:dataset, :ids)\"\n",
    "                ),\n",
    "                dict(dataset=dataset, ids=ids),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class InternalIds:\n",
    "    def set_desc_quant_from_label(self, label: str, desctriptor: str):\n",
    "        self.__setattr__(label, desctriptor)\n",
    "\n",
    "    def __init__(self, queries) -> None:\n",
    "\n",
    "        q = queries\n",
    "        self._q = queries\n",
    "\n",
    "        self.addr_suid = q.address_from_fadd_type_fadd(\n",
    "            \"tabular-header\", \"id_sub\"\n",
    "        )\n",
    "        self.addr_said = q.address_from_fadd_type_fadd(\n",
    "            \"tabular-header\", \"id_sam\"\n",
    "        )\n",
    "        self.addr_spec = q.address_from_fadd_type_fadd(\n",
    "            \"tabular-header\", \"species\"\n",
    "        )\n",
    "        self.addr_saty = q.address_from_fadd_type_fadd(\n",
    "            \"tabular-header\", \"sample_type\"\n",
    "        )\n",
    "        self.addr_faid = q.address_from_fadd_type_fadd(\n",
    "            \"tabular-header\", \"fascicle\"\n",
    "        )  # for REVA ft\n",
    "\n",
    "        self.addr_tmod = q.address_from_fadd_type_fadd(\n",
    "            \"tabular-header\", \"modality\"\n",
    "        )\n",
    "        # addr_trai = address_from_fadd_type_fadd('tabular-header', 'raw_anat_index')\n",
    "        # addr_tnai = address_from_fadd_type_fadd('tabular-header', 'norm_anat_index')\n",
    "        # addr_context = address_from_fadd_type_fadd('context', '#/path-metadata/{index of match remote_id}/dataset_relative_path')  # XXX this doesn't do what we want, I think what we really would want in these contexts are objects_internal that reference the file system state for a given updated snapshot, that is the real \"object\" that corresponds to the path-metadata.json that we are working from\n",
    "\n",
    "        # addr_jpmod = address_from_fadd_type_fadd('json-path-with-types', '#/#int/modality')\n",
    "        # addr_jprai = address_from_fadd_type_fadd('json-path-with-types', '#/#int/anat_index')\n",
    "        # addr_jpnai = address_from_fadd_type_fadd('json-path-with-types', '#/#int/norm_anat_index')\n",
    "\n",
    "        self.addr_jpdrp = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path\",\n",
    "        )\n",
    "\n",
    "        # XXX these are more accurate if opaque\n",
    "        self.addr_jpmod = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path#derive-modality\",\n",
    "        )\n",
    "        # addr_jprai = address_from_fadd_type_fadd('json-path-with-types', '#/path-metadata/data/#int/dataset_relative_path#derive-raw-anat-index')\n",
    "        self.addr_jpnai = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path#derive-norm-anat-index-v1\",\n",
    "        )\n",
    "        self.addr_jpnain = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path#derive-norm-anat-index-v1-min\",\n",
    "        )\n",
    "        self.addr_jpnaix = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path#derive-norm-anat-index-v1-max\",\n",
    "        )\n",
    "        self.addr_jpsuid = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path#derive-subject-id\",\n",
    "        )\n",
    "        self.addr_jpsaid = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\",\n",
    "            \"#/path-metadata/data/#int/dataset_relative_path#derive-sample-id\",\n",
    "        )\n",
    "\n",
    "        self.addr_jpspec = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\", \"#/local/tom-made-it-up/species\"\n",
    "        )\n",
    "        self.addr_jpsaty = q.address_from_fadd_type_fadd(\n",
    "            \"json-path-with-types\", \"#/local/tom-made-it-up/sample_type\"\n",
    "        )\n",
    "\n",
    "        # future version when we actually have the metadata files\n",
    "        # addr_jpmod = address_from_fadd_type_fadd('json-path-with-types', '#/curation-export/manifest/#int/modality')\n",
    "        # addr_jprai = address_from_fadd_type_fadd('json-path-with-types', '#/curation-export/samples/#int/raw_anat_index')\n",
    "        # addr_jpnai = address_from_fadd_type_fadd('json-path-with-types', '#/curation-export/samples/#int/norm_anat_index')\n",
    "        # addr_jpsuid = address_from_fadd_type_fadd('json-path-with-types', '#/curation-export/subjects/#int/id_sub')\n",
    "        # addr_jpsaid = address_from_fadd_type_fadd('json-path-with-types', '#/curation-export/samples/#int/id_sam')\n",
    "\n",
    "        self.addr_const_null = q.address_from_fadd_type_fadd(\"constant\", None)\n",
    "\n",
    "        # qd_rai = desc_quant_from_label('reva ft sample anatomical location distance index raw')\n",
    "        self.qd_nai = q.desc_quant_from_label(\n",
    "            \"reva ft sample anatomical location distance index normalized v1\"\n",
    "        )\n",
    "        self.qd_nain = q.desc_quant_from_label(\n",
    "            \"reva ft sample anatomical location distance index normalized v1 min\"\n",
    "        )\n",
    "        self.qd_naix = q.desc_quant_from_label(\n",
    "            \"reva ft sample anatomical location distance index normalized v1 max\"\n",
    "        )\n",
    "\n",
    "        self.cd_mod = q.desc_cat_from_label_domain_label(\n",
    "            \"hasDataAboutItModality\", None\n",
    "        )\n",
    "        self.cd_bot = q.desc_cat_from_label_domain_label(\n",
    "            \"bottom\", None\n",
    "        )  # we just need something we can reference that points to null so we can have a refernce to all the objects\n",
    "\n",
    "        self.id_human = q.desc_inst_from_label(\"human\")\n",
    "        self.id_nerve = q.desc_inst_from_label(\"nerve\")\n",
    "        self.id_nerve_volume = q.desc_inst_from_label(\"nerve-volume\")\n",
    "        self.luid = {\n",
    "            \"human\": self.id_human,\n",
    "            \"nerve\": self.id_nerve,\n",
    "            \"nerve-volume\": self.id_nerve_volume,\n",
    "        }\n",
    "\n",
    "        self.ct_mod = q.cterm_from_label(\"microct\")  # lol ct ct\n",
    "        self.ct_hack = q.cterm_from_label(\"hack-associate-some-value\")\n",
    "        self.luct = {\n",
    "            \"ct-hack\": self.ct_hack,\n",
    "            \"microct\": self.ct_mod,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Units(label='mm', iri='http://qudt.org/vocab/unit/MM')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Units(\"mm\", \"http://qudt.org/vocab/unit/MM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-12 07:32:03,543 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-09-12 07:32:03,544 INFO sqlalchemy.engine.Engine INSERT INTO units (label, iri) VALUES (%(label)s, %(iri)s)\n",
      "2024-09-12 07:32:03,544 INFO sqlalchemy.engine.Engine [cached since 5593s ago] {'label': 'pixel', 'iri': 'http://uri.interlex.org/tgbugs/uris/readable/aspect/unit/pixel'}\n",
      "Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"units_label_key\"\n",
      "DETAIL:  Key (label)=(pixel) already exists.\n",
      "\n",
      "[SQL: INSERT INTO units (label, iri) VALUES (%(label)s, %(iri)s)]\n",
      "[parameters: {'label': 'pixel', 'iri': 'http://uri.interlex.org/tgbugs/uris/readable/aspect/unit/pixel'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/gkpj)\n",
      "2024-09-12 07:32:03,636 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "2024-09-12 07:32:03,681 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-09-12 07:32:03,681 INFO sqlalchemy.engine.Engine INSERT INTO objects (id, id_type) VALUES (%(id)s, %(id_type)s) ON CONFLICT DO NOTHING\n",
      "2024-09-12 07:32:03,681 INFO sqlalchemy.engine.Engine [cached since 252.6s ago] {'id': 'aa43eda8-b29a-4c25-9840-ecbd57598afc', 'id_type': 'dataset'}\n",
      "2024-09-12 07:32:03,773 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import typing\n",
    "from typing import ClassVar, Dict, Protocol, Any\n",
    "\n",
    "type UUID = str\n",
    "\n",
    "\n",
    "class Dataclass(Protocol):\n",
    "    __dataclass_fields__: ClassVar[Dict[str, Any]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Unit:\n",
    "    __table__ = \"units\"\n",
    "    label: str\n",
    "    iri: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Objects:\n",
    "    __table__ = \"objects\"\n",
    "    id: UUID\n",
    "    id_type: str\n",
    "    id_file: int | None = None\n",
    "    id_internal: UUID | None = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DescriptorQuant:\n",
    "    __table__ = \"descriptors_quant\"\n",
    "    label: str\n",
    "    iri: str\n",
    "\n",
    "\n",
    "class Ingest:\n",
    "\n",
    "    def __init__(self, session: Session):\n",
    "        self.session = session\n",
    "\n",
    "    def execute_insert(\n",
    "        self, query: str, dataclasses: typing.Sequence[Dataclass]\n",
    "    ) -> None:\n",
    "        \"\"\"Execute a query with dataclasses as parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            sql query string\n",
    "        dataclasses : typing.Sequence[Dataclass]\n",
    "            a list of SQL table dataclasses to be inserted\n",
    "        \"\"\"\n",
    "        statement = sql_text(query)\n",
    "        params = [asdict(dataclass) for dataclass in dataclasses]\n",
    "        try:\n",
    "            self.session.execute(statement=statement, params=params)\n",
    "            self.session.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            self.session.rollback()\n",
    "\n",
    "    def insert_units(self, units: typing.Sequence[Unit]) -> None:\n",
    "        query = r\"INSERT INTO units (label, iri) VALUES (:label, :iri)\"\n",
    "        self.execute_insert(query=query, dataclasses=units)\n",
    "\n",
    "    def insert_objects(self, objects: typing.Sequence[Objects]) -> None:\n",
    "        query = r\"INSERT INTO objects (id, id_type) VALUES (:id, :id_type) ON CONFLICT DO NOTHING\"\n",
    "        self.execute_insert(query=query, dataclasses=objects)\n",
    "\n",
    "    def insert_descriptors_quant(\n",
    "        self, descriptors: typing.Sequence[DescriptorQuant]\n",
    "    ) -> None:\n",
    "        query = (\n",
    "            r\"INSERT INTO descriptors_quant (label, iri) VALUES (:label, :iri)\"\n",
    "        )\n",
    "        self.execute_insert(query=query, dataclasses=descriptors)\n",
    "\n",
    "\n",
    "ingest = Ingest(session=session)\n",
    "\n",
    "ingest.insert_units(\n",
    "    units=[\n",
    "        Unit(\n",
    "            label=\"pixel\",\n",
    "            iri=\"http://uri.interlex.org/tgbugs/uris/readable/aspect/unit/pixel\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ingest.insert_objects(\n",
    "    objects=[\n",
    "        Objects(id=dataset[\"content\"][\"id\"].split(\":\")[-1], id_type=\"dataset\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N:dataset:aa43eda8-b29a-4c25-9840-ecbd57598afc'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"content\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 32 (4174477168.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[54], line 36\u001b[0;36m\u001b[0m\n\u001b[0;31m    Ingests(dataset).extract()\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 32\n"
     ]
    }
   ],
   "source": [
    "# wrap columns\n",
    "q = Queries(session)\n",
    "i = InternalIds(q)\n",
    "\n",
    "# updated_transitive,\n",
    "# values_objects,\n",
    "# values_dataset_object,\n",
    "# make_values_instances,\n",
    "# make_values_parents,\n",
    "# make_void,\n",
    "# make_vocd,\n",
    "# make_voqd,\n",
    "# make_values_cat,\n",
    "# make_values_quant,\n",
    "\n",
    "\n",
    "class Ingests:\n",
    "    \"\"\"Create ingest objects for a dataset using Pennsieive direct\"\"\"\n",
    "\n",
    "    def __init__(self, dataset: str) -> None:\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def extract(self):\n",
    "        return (self.updated_transitive(), self.values_objects())\n",
    "\n",
    "    def updated_transitive(self):\n",
    "        \"\"\"Last updated Timestamp\"\"\"\n",
    "        return max(\n",
    "            [v[\"content\"][\"updatedAt\"] for v in self.dataset[\"children\"]]\n",
    "        )\n",
    "    \n",
    "    def values_objects(self):\n",
    "        \n",
    "        \n",
    "Ingests(dataset).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from functools import cache\n",
    "from pathlib import Path\n",
    "from pydantic import BaseSettings, BaseModel\n",
    "import boto3\n",
    "from botocore.client import BaseClient\n",
    "import requests\n",
    "from typing import Tuple, List, Any\n",
    "\n",
    "\n",
    "class PennsieveModel(BaseSettings):\n",
    "    PENNSIEVE_API_TOKEN: str\n",
    "    PENNSIEVE_API_SECRET: str\n",
    "    auth_url: str = \"https://api.pennsieve.io/authentication/cognito-config\"\n",
    "    private_dataset_url: str = \"https://api.pennsieve.io/datasets/\"\n",
    "    public_dataset_url: str = \"https://api.pennsieve.io/discover/datasets\"\n",
    "\n",
    "    class Config:  # type: ignore\n",
    "\n",
    "        env_file = Path.home() / \".scicrunch/credentials/pennsieve\"\n",
    "        fields = {\n",
    "            \"PENNSIEVE_API_TOKEN\": {\"env\": \"PENNSIEVE_API_TOKEN\"},\n",
    "            \"PENNSIEVE_API_SECRET\": {\"env\": \"PENNSIEVE_API_SECRET\"},\n",
    "        }\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    pennsieve: PennsieveModel = PennsieveModel()\n",
    "\n",
    "\n",
    "class PennsieveClient:\n",
    "    def __init__(self):\n",
    "        settings = Settings().pennsieve\n",
    "        self.public_dataset_url = settings.public_dataset_url\n",
    "        self.private_dataset_url = settings.private_dataset_url\n",
    "        self.auth_url = settings.auth_url\n",
    "        self.api_key = self.__aws_bot_login(\n",
    "            token=settings.PENNSIEVE_API_TOKEN,\n",
    "            secret=settings.PENNSIEVE_API_SECRET,\n",
    "        )\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"accept\": \"*/*\",\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def __reauth(self) -> None:\n",
    "        settings = Settings().pennsieve\n",
    "        self.api_key = self.__aws_bot_login(\n",
    "            token=settings.PENNSIEVE_API_TOKEN,\n",
    "            secret=settings.PENNSIEVE_API_SECRET,\n",
    "        )\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"accept\": \"*/*\",\n",
    "        }\n",
    "\n",
    "    def __aws_bot_login(self, token: str, secret: str) -> str:\n",
    "        \"\"\"Awful AWS bot login for real API key using token + secrets\"\"\"\n",
    "        r = requests.get(self.auth_url)\n",
    "        r.raise_for_status()\n",
    "        cognito_app_client_id = r.json()[\"tokenPool\"][\"appClientId\"]\n",
    "        cognito_region = r.json()[\"region\"]\n",
    "        cognito_idp_client: BaseClient = boto3.client(  # type: ignore\n",
    "            \"cognito-idp\",\n",
    "            region_name=cognito_region,\n",
    "            aws_access_key_id=\"\",\n",
    "            aws_secret_access_key=\"\",\n",
    "        )\n",
    "        login_response = cognito_idp_client.initiate_auth(  # type: ignore\n",
    "            AuthFlow=\"USER_PASSWORD_AUTH\",\n",
    "            AuthParameters={\"USERNAME\": token, \"PASSWORD\": secret},\n",
    "            ClientId=cognito_app_client_id,\n",
    "        )\n",
    "        api_key: str = login_response[\"AuthenticationResult\"][\"AccessToken\"]\n",
    "        return api_key\n",
    "\n",
    "    def __get(\n",
    "        self,\n",
    "        url: str,\n",
    "        params: dict[str, Any] | None = None,\n",
    "    ) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Get request to endpoint with params\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        endpoint : URL\n",
    "            API endpoint to hit\n",
    "        params : dict, optional\n",
    "            API options, by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            API json response\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    def _post(\n",
    "        self,\n",
    "        url: str,\n",
    "        data: dict[str, Any] | None = None,\n",
    "        payload: dict[str, Any] | None = None,\n",
    "    ) -> requests.Response:\n",
    "        \"\"\"\n",
    "        Post request to endpoint with data or json payload\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        endpoint : URL\n",
    "            API endpoint to hit\n",
    "        data : dict, optional\n",
    "            API contents, by default None\n",
    "        json : dict, optional\n",
    "            API contents, by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            API json response\n",
    "        \"\"\"\n",
    "        response = self.session.post(\n",
    "            url, data=data, json=payload, headers=self.headers\n",
    "        )\n",
    "        # Pennsieve API is unstable, only fail on 500s\n",
    "        if response.status_code >= 400:\n",
    "            print(response.text)\n",
    "            self.__reauth()\n",
    "            response = self.session.post(\n",
    "                url, data=data, json=payload, headers=self.headers\n",
    "            )\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    @staticmethod\n",
    "    def create_path(path: Path) -> None:\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=False)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "    def get_user(self):\n",
    "        return self.__get(\"https://api.pennsieve.io/user/\").json()\n",
    "\n",
    "    @cache\n",
    "    def get_dataset(self, id_or_name: str) -> dict[str, Any]:\n",
    "        return self.__get(\n",
    "            f\"https://api.pennsieve.io/datasets/{id_or_name}\"\n",
    "        ).json()\n",
    "\n",
    "    def get_dataset_packages(self, id_or_name: str) -> dict[str, Any]:\n",
    "        return self.__get(\n",
    "            f\"https://api.pennsieve.io/datasets/{id_or_name}/packages\"\n",
    "        ).json()[\"packages\"]\n",
    "\n",
    "    @cache\n",
    "    def get_dataset_manifest(self, id_or_name: str) -> dict[str, Any]:\n",
    "        url = \"https://api.pennsieve.io/packages/download-manifest\"\n",
    "        dataset = self.get_dataset(id_or_name)\n",
    "        payload = {\n",
    "            \"nodeIds\": [\n",
    "                child[\"content\"][\"nodeId\"] for child in dataset[\"children\"]\n",
    "            ]\n",
    "        }\n",
    "        dataset[\"files\"] = self._post(url, payload=payload).json()\n",
    "        return dataset\n",
    "\n",
    "    @cache\n",
    "    def get_child_manifest(self, node_id: str) -> dict[str, Any]:\n",
    "        url = \"https://api.pennsieve.io/packages/download-manifest\"\n",
    "        payload = {\"nodeIds\": [node_id]}\n",
    "        resp = self._post(url, payload=payload)\n",
    "        return resp.json()\n",
    "\n",
    "    def export_dataset(\n",
    "        self, id_or_name: str, output_dir: Path | str, verbose: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"Export a dataset to a directory\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        id_or_name : str\n",
    "            Dataset ID or name\n",
    "        output_dir : Path | str\n",
    "            Output directory\n",
    "        verbose : bool, optional\n",
    "            Prints filename being downloaded, by default True\n",
    "        \"\"\"\n",
    "        url = \"https://api.pennsieve.io/packages/download-manifest\"\n",
    "        output_dir = Path(output_dir) / id_or_name\n",
    "        # Pull dataset for root children IDs\n",
    "        dataset = self.get_dataset(id_or_name)\n",
    "        for child in dataset[\"children\"]:\n",
    "            payload = {\"nodeIds\": [child[\"content\"][\"nodeId\"]]}\n",
    "            # Pull tree for 1 child at a time since prebuilt s3 links only last a couple hours\n",
    "            # If all children are pulled at once, the links will expire before all files are downloaded if over 200GBs\n",
    "            manifest = self._post(url, payload=payload).json()\n",
    "            for filemeta in manifest[\"data\"]:\n",
    "                parents_path = output_dir / \"/\".join(filemeta[\"path\"])\n",
    "                # Create nested parent directories if they don\"t exist\n",
    "                self.create_path(parents_path)\n",
    "                file_path = parents_path / filemeta[\"fileName\"]\n",
    "                if verbose:\n",
    "                    print(f\"downloading path: {file_path}\")\n",
    "                url = filemeta[\"url\"]\n",
    "                # If file already exists, skip if it is stopped in the middle of downloading\n",
    "                # TODO: query file for checksum; very slow but will garuntee no partial downloads\n",
    "                if file_path.exists():\n",
    "                    continue\n",
    "                # Stream download to file\n",
    "                with requests.get(url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(file_path, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "\n",
    "    def _private_datasets(self):\n",
    "        \"\"\"Get Private dataset for it\"s N:# ID\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        There is no known limiter at the moment. Expect all results and be conservative.\n",
    "        \"\"\"\n",
    "        return self.__get(self.private_dataset_url).json()\n",
    "\n",
    "    def _public_datasets(\n",
    "        self, limit: int = 1, offset: int = 0, asc: bool = False\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Everything about the dataset except the N:# id itself\"\"\"\n",
    "        params = {\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "            \"orderBy\": \"date\",\n",
    "            \"orderDirection\": \"asc\" if asc else \"desc\",\n",
    "        }\n",
    "        return self.__get(self.public_dataset_url, params=params).json()[\n",
    "            \"datasets\"\n",
    "        ]\n",
    "\n",
    "    def get_datasets(self) -> List[dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get all complete datasets\n",
    "        \"\"\"\n",
    "        # Private API endpoint Needs to be normalized via content key\n",
    "        intId_id = {\n",
    "            d[\"content\"][\"intId\"]: d[\"content\"][\"id\"]\n",
    "            for d in self._private_datasets()\n",
    "        }\n",
    "        datasets = self._public_datasets(limit=100000)\n",
    "        for dataset in datasets:\n",
    "            dataset[\"dataset_id\"] = intId_id.get(dataset[\"sourceDatasetId\"])\n",
    "        return datasets\n",
    "\n",
    "    def get_partial_hash(self) -> Tuple[int, int, int | None]:\n",
    "        \"\"\"Get latest dataset id & version to see if anything new from pennsieve is worth pulling\"\"\"\n",
    "        latest_dataset = self._public_datasets()[0]\n",
    "        partial_hash = (\n",
    "            latest_dataset[\"sourceDatasetId\"],\n",
    "            latest_dataset[\"version\"],\n",
    "            latest_dataset[\"revision\"],\n",
    "        )\n",
    "        return partial_hash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
